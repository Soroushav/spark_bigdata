{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4067b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19f504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/11 16:42:00 WARN Utils: Your hostname, Soroushs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.101 instead (on interface en0)\n",
      "25/11/11 16:42:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 16:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('MlLib').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58a2b8",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbace78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+------+-------------+------+\n",
      "|customer_id|age|income|gender|loyalty_score|region|\n",
      "+-----------+---+------+------+-------------+------+\n",
      "|        754| 56| 23343|Female|            9|  West|\n",
      "|        214| 69| 33500|Female|            1| South|\n",
      "|        125| 46| 73222|  Male|            9|  East|\n",
      "|        859| 32| 49375|Female|            8|  East|\n",
      "|        381| 60| 29662|Female|            4| North|\n",
      "+-----------+---+------+------+-------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path='datasets/customer_data.csv', header=True, inferSchema=True)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21612002",
   "metadata": {},
   "source": [
    "### Use VectorAssembler to combine selected columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3de82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id', 'age', 'income', 'gender', 'loyalty_score', 'region']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb11c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+\n",
      "|customer_id|age|income|gender|loyalty_score|region|gender_index|   gender_ohe|            features|\n",
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+\n",
      "|        754| 56| 23343|Female|            9|  West|         0.0|(3,[0],[1.0])|[56.0,23343.0,1.0...|\n",
      "|        214| 69| 33500|Female|            1| South|         0.0|(3,[0],[1.0])|[69.0,33500.0,1.0...|\n",
      "|        125| 46| 73222|  Male|            9|  East|         1.0|(3,[1],[1.0])|[46.0,73222.0,0.0...|\n",
      "|        859| 32| 49375|Female|            8|  East|         0.0|(3,[0],[1.0])|[32.0,49375.0,1.0...|\n",
      "|        381| 60| 29662|Female|            4| North|         0.0|(3,[0],[1.0])|[60.0,29662.0,1.0...|\n",
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([56.0, 23343.0, 1.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Encode gender (string -> index -> one-hot)\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\", handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"gender_index\"], outputCols=[\"gender_ohe\"])\n",
    "\n",
    "# Assemble features (use the OHE vector instead of raw string)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"income\", \"gender_ohe\"],\n",
    "    outputCol=\"features\"    \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "model = pipeline.fit(df)\n",
    "output = model.transform(df)\n",
    "output.show(n=5)\n",
    "output.first()['features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ea2ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+-----+\n",
      "|customer_id|age|income|gender|loyalty_score|region|gender_index|   gender_ohe|            features|label|\n",
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+-----+\n",
      "|        754| 56| 23343|Female|            9|  West|         0.0|(3,[0],[1.0])|[56.0,23343.0,1.0...|    1|\n",
      "|        214| 69| 33500|Female|            1| South|         0.0|(3,[0],[1.0])|[69.0,33500.0,1.0...|    0|\n",
      "|        125| 46| 73222|  Male|            9|  East|         1.0|(3,[1],[1.0])|[46.0,73222.0,0.0...|    1|\n",
      "|        859| 32| 49375|Female|            8|  East|         0.0|(3,[0],[1.0])|[32.0,49375.0,1.0...|    1|\n",
      "|        381| 60| 29662|Female|            4| North|         0.0|(3,[0],[1.0])|[60.0,29662.0,1.0...|    0|\n",
      "+-----------+---+------+------+-------------+------+------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "output = output.withColumn(\"label\", when(col(\"loyalty_score\") >= 7, 1).otherwise(0))\n",
    "output.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a25c57",
   "metadata": {},
   "source": [
    "### Select only the feature vector and label for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33831cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[56.0,23343.0,1.0...|    1|\n",
      "|[69.0,33500.0,1.0...|    0|\n",
      "|[46.0,73222.0,0.0...|    1|\n",
      "|[32.0,49375.0,1.0...|    1|\n",
      "|[60.0,29662.0,1.0...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "output = output.select('features', 'label')\n",
    "output.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56e07a",
   "metadata": {},
   "source": [
    "### partition data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0bf5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = output.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd16210",
   "metadata": {},
   "source": [
    "### Define and fit a logistic regression model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2f5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "regressor = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "model = regressor.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ddd84",
   "metadata": {},
   "source": [
    "### Apply the trained model to the test data to predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd5cfd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+------------------------------------------+\n",
      "|label|prediction|probability                             |rawPrediction                             |\n",
      "+-----+----------+----------------------------------------+------------------------------------------+\n",
      "|0    |0.0       |[0.5536795632131665,0.44632043678683353]|[0.21554895147643272,-0.21554895147643272]|\n",
      "|0    |0.0       |[0.5981997337266606,0.4018002662733394] |[0.39796958598966925,-0.39796958598966925]|\n",
      "|0    |0.0       |[0.585305799704103,0.414694200295897]   |[0.34459306333660533,-0.34459306333660533]|\n",
      "|0    |0.0       |[0.581447979529271,0.41855202047072904] |[0.32872032444527854,-0.32872032444527854]|\n",
      "|0    |0.0       |[0.6078345557402057,0.39216544425979427]|[0.43821893023265623,-0.43821893023265623]|\n",
      "|1    |0.0       |[0.5798950821130343,0.4201049178869657] |[0.3223427096945656,-0.3223427096945656]  |\n",
      "|1    |0.0       |[0.5775922097453148,0.4224077902546852] |[0.3128969251448388,-0.3128969251448388]  |\n",
      "|1    |0.0       |[0.6198624644659321,0.3801375355340679] |[0.4889644990708235,-0.4889644990708235]  |\n",
      "|0    |0.0       |[0.6137911662317023,0.38620883376829773]|[0.46327650685802907,-0.46327650685802907]|\n",
      "|0    |0.0       |[0.582636066772091,0.417363933227909]   |[0.3336041664517012,-0.3336041664517012]  |\n",
      "|0    |0.0       |[0.5519735190517886,0.4480264809482114] |[0.20864773246659812,-0.20864773246659812]|\n",
      "|1    |0.0       |[0.6047846358306158,0.3952153641693842] |[0.4254415787585771,-0.4254415787585771]  |\n",
      "|0    |0.0       |[0.6018001880129481,0.3981998119870519] |[0.4129715571406493,-0.4129715571406493]  |\n",
      "|1    |0.0       |[0.6274305050143769,0.3725694949856231] |[0.5212093317531837,-0.5212093317531837]  |\n",
      "|0    |0.0       |[0.5922179271429534,0.4077820728570466] |[0.3731417907694403,-0.3731417907694403]  |\n",
      "|1    |0.0       |[0.586247453048367,0.413752546951633]   |[0.348473892823989,-0.348473892823989]    |\n",
      "|1    |0.0       |[0.5840161576610526,0.41598384233894736]|[0.3392822306977523,-0.3392822306977523]  |\n",
      "|0    |0.0       |[0.5980760487575706,0.4019239512424294] |[0.39745502299125707,-0.39745502299125707]|\n",
      "|0    |0.0       |[0.6025557665624464,0.39744423343755364]|[0.41612558867788485,-0.41612558867788485]|\n",
      "|0    |0.0       |[0.5682480219523379,0.4317519780476621] |[0.2747066832987859,-0.2747066832987859]  |\n",
      "+-----+----------+----------------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_set)\n",
    "predictions.select(\"label\", \"prediction\", \"probability\", 'rawPrediction').show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac75ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC (Area Under ROC): 0.500\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#  Initialize the BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",           # your actual label column\n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"   # you can also use \"areaUnderPR\"\n",
    ")\n",
    "\n",
    "#  Evaluate the model\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC (Area Under ROC): {roc_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d97221",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/logistic_regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
